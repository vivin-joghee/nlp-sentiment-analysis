{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd98f3e",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Translation Experiments\n",
    "\n",
    "This notebook implements various encoder-decoder architectures for French-to-English translation:\n",
    "\n",
    "1. **Baseline**: GRU Encoder + GRU Decoder\n",
    "2. **Experiment 1**: LSTM Encoder + LSTM Decoder\n",
    "3. **Experiment 2**: Bi-LSTM Encoder + GRU Decoder\n",
    "4. **Experiment 3**: GRU Encoder + GRU Decoder with Attention\n",
    "5. **Experiment 4**: Transformer Encoder + GRU Decoder\n",
    "\n",
    "We will record and compare Rouge scores for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d4dca",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dba691",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"Language vocabulary class for mapping words to indices and vice versa.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7be0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"Convert Unicode string to plain ASCII.\"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters.\"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    \"\"\"Read translation pairs from file.\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f06746",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am\", \"i m\",\n",
    "    \"he is\", \"he s\",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re\",\n",
    "    \"we are\", \"we re\",\n",
    "    \"they are\", \"they re\"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"Filter pairs by length and prefix.\"\"\"\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    \"\"\"Prepare data: read, filter, and build vocabulary.\"\"\"\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(f\"\\nExample pair: {random.choice(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = [i[0] for i in pairs]\n",
    "y = [i[1] for i in pairs]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_pairs = list(zip(X_train, y_train))\n",
    "test_pairs = list(zip(X_test, y_test))\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ce519",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1ecb7",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c51bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH, use_attention=False):\n",
    "    \"\"\"Evaluate a single sentence.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            if use_attention:\n",
    "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d490cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, testing_pairs, use_attention=False):\n",
    "    \"\"\"Run inference on test pairs.\"\"\"\n",
    "    inputs = []\n",
    "    gt = []\n",
    "    predict = []\n",
    "\n",
    "    for i in tqdm(range(len(testing_pairs))):\n",
    "        pair = testing_pairs[i]\n",
    "        output_words = evaluate(encoder, decoder, pair[0], use_attention=use_attention)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        inputs.append(pair[0])\n",
    "        gt.append(pair[1])\n",
    "        predict.append(output_sentence)\n",
    "\n",
    "    return inputs, gt, predict\n",
    "\n",
    "def eval_rouge(gt, predict):\n",
    "    \"\"\"Calculate and print Rouge scores.\"\"\"\n",
    "    rouge = ROUGEScore()\n",
    "    metric_score = rouge(predict, gt)\n",
    "    print(\"=\" * 45)\n",
    "    print(\"         Evaluation Score - ROUGE Score\")\n",
    "    print(\"=\" * 45)\n",
    "    print(f\"Rouge1 F-measure:  {metric_score['rouge1_fmeasure'].item():.4f}\")\n",
    "    print(f\"Rouge1 Precision:  {metric_score['rouge1_precision'].item():.4f}\")\n",
    "    print(f\"Rouge1 Recall:     {metric_score['rouge1_recall'].item():.4f}\")\n",
    "    print(f\"Rouge2 F-measure:  {metric_score['rouge2_fmeasure'].item():.4f}\")\n",
    "    print(f\"Rouge2 Precision:  {metric_score['rouge2_precision'].item():.4f}\")\n",
    "    print(f\"Rouge2 Recall:     {metric_score['rouge2_recall'].item():.4f}\")\n",
    "    print(\"=\" * 45)\n",
    "    return metric_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e890ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fff440",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 2: Baseline - GRU Encoder + GRU Decoder\n",
    "\n",
    "This is the original implementation using GRU units for both encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    \"\"\"GRU-based Encoder.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    \"\"\"GRU-based Decoder.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train_step(input_tensor, target_tensor, encoder, decoder, \n",
    "               encoder_optimizer, decoder_optimizer, criterion, \n",
    "               max_length=MAX_LENGTH, use_attention=False):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            if use_attention:\n",
    "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            if use_attention:\n",
    "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e333c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, epochs, print_every=1000, \n",
    "               learning_rate=0.01, use_attention=False):\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    iter_count = 1\n",
    "    n_iters = len(train_pairs) * epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        for training_pair in train_pairs:\n",
    "            training_pair = tensorsFromPair(training_pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train_step(input_tensor, target_tensor, encoder, decoder,\n",
    "                             encoder_optimizer, decoder_optimizer, criterion,\n",
    "                             use_attention=use_attention)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if iter_count % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_count / n_iters),\n",
    "                                            iter_count, iter_count / n_iters * 100, print_loss_avg))\n",
    "            iter_count += 1\n",
    "    \n",
    "    print(f\"\\nTraining completed in {asMinutes(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline: GRU Encoder + GRU Decoder\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE: GRU Encoder + GRU Decoder\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hidden_size = 256\n",
    "encoder_gru = EncoderGRU(input_lang.n_words, hidden_size).to(device)\n",
    "decoder_gru = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(encoder_gru, decoder_gru, epochs=5, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed331f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Baseline\n",
    "print(\"\\nEvaluating Baseline (GRU + GRU)...\")\n",
    "_, gt, predict = inference(encoder_gru, decoder_gru, test_pairs)\n",
    "results['Baseline_GRU'] = eval_rouge(gt, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de889b8e",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 3: LSTM Encoder + LSTM Decoder\n",
    "\n",
    "Replace GRU with LSTM in both encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based Encoder.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # LSTM requires (h_0, c_0)\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based Decoder.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77da310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_lstm(input_tensor, target_tensor, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"Training step for LSTM models.\"\"\"\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters_lstm(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
    "    \"\"\"Training loop for LSTM models.\"\"\"\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    iter_count = 1\n",
    "    n_iters = len(train_pairs) * epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        for training_pair in train_pairs:\n",
    "            training_pair = tensorsFromPair(training_pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train_step_lstm(input_tensor, target_tensor, encoder, decoder,\n",
    "                                   encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if iter_count % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_count / n_iters),\n",
    "                                            iter_count, iter_count / n_iters * 100, print_loss_avg))\n",
    "            iter_count += 1\n",
    "    \n",
    "    print(f\"\\nTraining completed in {asMinutes(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8599606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"Evaluate for LSTM models.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "def inference_lstm(encoder, decoder, testing_pairs):\n",
    "    \"\"\"Inference for LSTM models.\"\"\"\n",
    "    inputs = []\n",
    "    gt = []\n",
    "    predict = []\n",
    "\n",
    "    for i in tqdm(range(len(testing_pairs))):\n",
    "        pair = testing_pairs[i]\n",
    "        output_words = evaluate_lstm(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        inputs.append(pair[0])\n",
    "        gt.append(pair[1])\n",
    "        predict.append(output_sentence)\n",
    "\n",
    "    return inputs, gt, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a09008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Encoder + LSTM Decoder\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 1: LSTM Encoder + LSTM Decoder\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_lstm = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
    "decoder_lstm = DecoderLSTM(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters_lstm(encoder_lstm, decoder_lstm, epochs=5, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "print(\"\\nEvaluating LSTM Encoder + LSTM Decoder...\")\n",
    "_, gt, predict = inference_lstm(encoder_lstm, decoder_lstm, test_pairs)\n",
    "results['LSTM_LSTM'] = eval_rouge(gt, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ba729",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4: Bi-LSTM Encoder + GRU Decoder\n",
    "\n",
    "Replace the encoder with Bidirectional LSTM while keeping GRU decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiLSTM(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM Encoder.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n",
    "        # Project bidirectional output back to hidden_size\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # Combine forward and backward outputs\n",
    "        output = self.fc(output)\n",
    "        # Combine forward and backward hidden states\n",
    "        h_n = hidden[0]  # (2, 1, hidden_size)\n",
    "        c_n = hidden[1]  # (2, 1, hidden_size)\n",
    "        # Concatenate and project back\n",
    "        h_combined = torch.cat((h_n[0], h_n[1]), dim=1).unsqueeze(0)  # (1, 1, hidden_size*2)\n",
    "        h_combined = self.fc(h_combined)\n",
    "        return output, h_combined\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Bidirectional LSTM: 2 directions\n",
    "        return (torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_bilstm(input_tensor, target_tensor, encoder, decoder, \n",
    "                      encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"Training step for BiLSTM encoder + GRU decoder.\"\"\"\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder.initHidden())\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    # encoder_hidden is already projected to correct size\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters_bilstm(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
    "    \"\"\"Training loop for BiLSTM encoder.\"\"\"\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    iter_count = 1\n",
    "    n_iters = len(train_pairs) * epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        for training_pair in train_pairs:\n",
    "            training_pair = tensorsFromPair(training_pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train_step_bilstm(input_tensor, target_tensor, encoder, decoder,\n",
    "                                     encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if iter_count % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_count / n_iters),\n",
    "                                            iter_count, iter_count / n_iters * 100, print_loss_avg))\n",
    "            iter_count += 1\n",
    "    \n",
    "    print(f\"\\nTraining completed in {asMinutes(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bilstm(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"Evaluate for BiLSTM encoder + GRU decoder.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder.initHidden())\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "def inference_bilstm(encoder, decoder, testing_pairs):\n",
    "    \"\"\"Inference for BiLSTM encoder.\"\"\"\n",
    "    inputs = []\n",
    "    gt = []\n",
    "    predict = []\n",
    "\n",
    "    for i in tqdm(range(len(testing_pairs))):\n",
    "        pair = testing_pairs[i]\n",
    "        output_words = evaluate_bilstm(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        inputs.append(pair[0])\n",
    "        gt.append(pair[1])\n",
    "        predict.append(output_sentence)\n",
    "\n",
    "    return inputs, gt, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bi-LSTM Encoder + GRU Decoder\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 2: Bi-LSTM Encoder + GRU Decoder\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_bilstm = EncoderBiLSTM(input_lang.n_words, hidden_size).to(device)\n",
    "decoder_gru2 = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters_bilstm(encoder_bilstm, decoder_gru2, epochs=5, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bi-LSTM\n",
    "print(\"\\nEvaluating Bi-LSTM Encoder + GRU Decoder...\")\n",
    "_, gt, predict = inference_bilstm(encoder_bilstm, decoder_gru2, test_pairs)\n",
    "results['BiLSTM_GRU'] = eval_rouge(gt, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ec2b2",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 5: GRU Encoder + GRU Decoder with Attention\n",
    "\n",
    "Add the attention mechanism between encoder and decoder.\n",
    "\n",
    "## Attention Mechanism\n",
    "\n",
    "At each decoder timestep $t$:\n",
    "\n",
    "1. **Compute attention scores**: $e_{t,i} = score(s_t, h_i)$\n",
    "2. **Normalize**: $\\alpha_{t,i} = softmax(e_{t,i})$\n",
    "3. **Context vector**: $c_t = \\sum_i \\alpha_{t,i} \\cdot h_i$\n",
    "4. **Combine**: $\\hat{s}_t = concat(s_t, c_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa633d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU Decoder with Bahdanau-style Attention.\n",
    "    \n",
    "    Attention Mechanism:\n",
    "    - Query: decoder hidden state s_t\n",
    "    - Keys/Values: encoder hidden states h_i\n",
    "    - Score function: additive (Bahdanau): v^T * tanh(W_s * s_t + W_h * h_i)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, max_length=MAX_LENGTH, dropout_p=0.1):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Attention layers (Bahdanau/Additive attention)\n",
    "        self.attn_W_s = nn.Linear(hidden_size, hidden_size)  # For decoder state\n",
    "        self.attn_W_h = nn.Linear(hidden_size, hidden_size)  # For encoder states\n",
    "        self.attn_v = nn.Linear(hidden_size, 1)  # For computing scalar score\n",
    "        \n",
    "        # GRU takes embedded input\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer takes concatenated [s_t; c_t]\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass with attention.\n",
    "        \n",
    "        Args:\n",
    "            input: Current input token (1,)\n",
    "            hidden: Previous decoder hidden state (1, 1, hidden_size)\n",
    "            encoder_outputs: All encoder hidden states (max_length, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: Log probabilities over vocabulary\n",
    "            hidden: Updated hidden state\n",
    "            attn_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Step 1: Embed input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Step 2: Run GRU to get decoder state s_t\n",
    "        gru_output, hidden = self.gru(embedded, hidden)\n",
    "        s_t = hidden.squeeze(0)  # (1, hidden_size)\n",
    "        \n",
    "        # Step 3: Compute attention scores using Bahdanau (additive) attention\n",
    "        # e_{t,i} = v^T * tanh(W_s * s_t + W_h * h_i)\n",
    "        \n",
    "        # Transform decoder state: W_s * s_t -> (1, hidden_size)\n",
    "        s_t_transformed = self.attn_W_s(s_t)  # (1, hidden_size)\n",
    "        \n",
    "        # Transform encoder outputs: W_h * h_i -> (max_length, hidden_size)\n",
    "        h_transformed = self.attn_W_h(encoder_outputs)  # (max_length, hidden_size)\n",
    "        \n",
    "        # Add and apply tanh: (max_length, hidden_size)\n",
    "        energy = torch.tanh(s_t_transformed + h_transformed)\n",
    "        \n",
    "        # Compute scalar scores: v^T * energy -> (max_length, 1)\n",
    "        attn_scores = self.attn_v(energy).squeeze(-1)  # (max_length,)\n",
    "        \n",
    "        # Step 4: Normalize scores to get attention distribution\n",
    "        attn_weights = F.softmax(attn_scores, dim=0)  # (max_length,)\n",
    "        \n",
    "        # Step 5: Compute context vector c_t = sum(alpha * h_i)\n",
    "        context = torch.sum(attn_weights.unsqueeze(1) * encoder_outputs, dim=0)  # (hidden_size,)\n",
    "        context = context.unsqueeze(0)  # (1, hidden_size)\n",
    "        \n",
    "        # Step 6: Combine decoder state with context: [s_t; c_t]\n",
    "        combined = torch.cat((s_t, context), dim=1)  # (1, hidden_size * 2)\n",
    "        \n",
    "        # Step 7: Compute output probabilities\n",
    "        output = self.softmax(self.out(combined))  # (1, output_size)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_attention(input_tensor, target_tensor, encoder, decoder, \n",
    "                         encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"Training step with attention.\"\"\"\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    # Encode all input tokens\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters_attention(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
    "    \"\"\"Training loop with attention.\"\"\"\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    iter_count = 1\n",
    "    n_iters = len(train_pairs) * epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        for training_pair in train_pairs:\n",
    "            training_pair = tensorsFromPair(training_pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train_step_attention(input_tensor, target_tensor, encoder, decoder,\n",
    "                                        encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if iter_count % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_count / n_iters),\n",
    "                                            iter_count, iter_count / n_iters * 100, print_loss_avg))\n",
    "            iter_count += 1\n",
    "    \n",
    "    print(f\"\\nTraining completed in {asMinutes(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attention(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"Evaluate with attention.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "def inference_attention(encoder, decoder, testing_pairs):\n",
    "    \"\"\"Inference with attention.\"\"\"\n",
    "    inputs = []\n",
    "    gt = []\n",
    "    predict = []\n",
    "\n",
    "    for i in tqdm(range(len(testing_pairs))):\n",
    "        pair = testing_pairs[i]\n",
    "        output_words = evaluate_attention(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        inputs.append(pair[0])\n",
    "        gt.append(pair[1])\n",
    "        predict.append(output_sentence)\n",
    "\n",
    "    return inputs, gt, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU Encoder + Attention Decoder\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 3: GRU Encoder + GRU Decoder with Attention\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_gru_attn = EncoderGRU(input_lang.n_words, hidden_size).to(device)\n",
    "decoder_attn = AttnDecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters_attention(encoder_gru_attn, decoder_attn, epochs=5, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Attention\n",
    "print(\"\\nEvaluating GRU Encoder + Attention Decoder...\")\n",
    "_, gt, predict = inference_attention(encoder_gru_attn, decoder_attn, test_pairs)\n",
    "results['GRU_Attention'] = eval_rouge(gt, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec4063",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 6: Transformer Encoder + GRU Decoder\n",
    "\n",
    "Replace the GRU encoder with a Transformer encoder. The sentence representation is the mean of all token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f448d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder.\n",
    "    \n",
    "    Processes the entire input sequence at once and returns:\n",
    "    - A sentence representation (mean of all token representations)\n",
    "    - All encoder outputs for potential attention\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Project to hidden size for decoder compatibility\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass - processes entire sequence at once.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input indices (seq_len, 1)\n",
    "            \n",
    "        Returns:\n",
    "            sentence_repr: Mean of all token representations (1, 1, hidden_size)\n",
    "            encoder_outputs: All encoder outputs (seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Embed: (seq_len, 1) -> (seq_len, 1, hidden_size)\n",
    "        embedded = self.embedding(input_tensor.squeeze(-1)).unsqueeze(1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Transformer encoding: (seq_len, 1, hidden_size)\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        \n",
    "        # Get sentence representation as mean of all tokens\n",
    "        # (seq_len, 1, hidden_size) -> (1, hidden_size)\n",
    "        sentence_repr = transformer_output.mean(dim=0)\n",
    "        sentence_repr = self.fc(sentence_repr)\n",
    "        sentence_repr = sentence_repr.unsqueeze(0)  # (1, 1, hidden_size)\n",
    "        \n",
    "        # Encoder outputs for each position: (seq_len, hidden_size)\n",
    "        encoder_outputs = transformer_output.squeeze(1)  # (seq_len, hidden_size)\n",
    "        \n",
    "        return sentence_repr, encoder_outputs\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Not used for Transformer, but kept for API compatibility\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_transformer(input_tensor, target_tensor, encoder, decoder, \n",
    "                           encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"Training step for Transformer encoder.\"\"\"\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    loss = 0\n",
    "\n",
    "    # Encode entire sequence at once\n",
    "    encoder_hidden, encoder_outputs_raw = encoder(input_tensor)\n",
    "    \n",
    "    # Pad encoder outputs to max_length\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    for ei in range(min(input_length, max_length)):\n",
    "        encoder_outputs[ei] = encoder_outputs_raw[ei]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters_transformer(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
    "    \"\"\"Training loop for Transformer encoder.\"\"\"\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate * 0.1)  # Lower LR for Transformer\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    iter_count = 1\n",
    "    n_iters = len(train_pairs) * epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        for training_pair in train_pairs:\n",
    "            training_pair = tensorsFromPair(training_pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train_step_transformer(input_tensor, target_tensor, encoder, decoder,\n",
    "                                          encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if iter_count % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter_count / n_iters),\n",
    "                                            iter_count, iter_count / n_iters * 100, print_loss_avg))\n",
    "            iter_count += 1\n",
    "    \n",
    "    print(f\"\\nTraining completed in {asMinutes(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"Evaluate for Transformer encoder.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        \n",
    "        # Encode entire sequence\n",
    "        encoder_hidden, _ = encoder(input_tensor)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "def inference_transformer(encoder, decoder, testing_pairs):\n",
    "    \"\"\"Inference for Transformer encoder.\"\"\"\n",
    "    inputs = []\n",
    "    gt = []\n",
    "    predict = []\n",
    "\n",
    "    for i in tqdm(range(len(testing_pairs))):\n",
    "        pair = testing_pairs[i]\n",
    "        output_words = evaluate_transformer(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        inputs.append(pair[0])\n",
    "        gt.append(pair[1])\n",
    "        predict.append(output_sentence)\n",
    "\n",
    "    return inputs, gt, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer Encoder + GRU Decoder\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 4: Transformer Encoder + GRU Decoder\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_transformer = TransformerEncoder(input_lang.n_words, hidden_size, nhead=4, num_layers=2).to(device)\n",
    "decoder_gru_trans = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters_transformer(encoder_transformer, decoder_gru_trans, epochs=5, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transformer\n",
    "print(\"\\nEvaluating Transformer Encoder + GRU Decoder...\")\n",
    "_, gt, predict = inference_transformer(encoder_transformer, decoder_gru_trans, test_pairs)\n",
    "results['Transformer_GRU'] = eval_rouge(gt, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb29a5c",
   "metadata": {},
   "source": [
    "---\n",
    "# Results Summary\n",
    "\n",
    "Compare all models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e51fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name, scores in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Rouge-1 F1': f\"{scores['rouge1_fmeasure'].item():.4f}\",\n",
    "        'Rouge-1 Precision': f\"{scores['rouge1_precision'].item():.4f}\",\n",
    "        'Rouge-1 Recall': f\"{scores['rouge1_recall'].item():.4f}\",\n",
    "        'Rouge-2 F1': f\"{scores['rouge2_fmeasure'].item():.4f}\",\n",
    "        'Rouge-2 Precision': f\"{scores['rouge2_precision'].item():.4f}\",\n",
    "        'Rouge-2 Recall': f\"{scores['rouge2_recall'].item():.4f}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                         RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = list(results.keys())\n",
    "rouge1_f1 = [results[m]['rouge1_fmeasure'].item() for m in models]\n",
    "rouge2_f1 = [results[m]['rouge2_fmeasure'].item() for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, rouge1_f1, width, label='Rouge-1 F1', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, rouge2_f1, width, label='Rouge-2 F1', color='coral')\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Model Comparison - Rouge Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabc138",
   "metadata": {},
   "source": [
    "---\n",
    "# Detailed Results Analysis and Report\n",
    "\n",
    "This section provides comprehensive analysis, comparison, and explanation of the experimental findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e3068",
   "metadata": {},
   "source": [
    "## Table 1: Complete Rouge Score Comparison\n",
    "\n",
    "This table summarizes all Rouge metrics across all model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a941a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Comprehensive Results Summary Table\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_results_table(results):\n",
    "    \"\"\"Create a formatted HTML table of all results.\"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    table_data = []\n",
    "    for model_name, scores in results.items():\n",
    "        table_data.append({\n",
    "            'Model': model_name.replace('_', ' '),\n",
    "            'Rouge-1 F1': scores['rouge1_fmeasure'].item(),\n",
    "            'Rouge-1 Prec': scores['rouge1_precision'].item(),\n",
    "            'Rouge-1 Rec': scores['rouge1_recall'].item(),\n",
    "            'Rouge-2 F1': scores['rouge2_fmeasure'].item(),\n",
    "            'Rouge-2 Prec': scores['rouge2_precision'].item(),\n",
    "            'Rouge-2 Rec': scores['rouge2_recall'].item(),\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Find best values for highlighting\n",
    "    best_r1_f1 = df['Rouge-1 F1'].max()\n",
    "    best_r2_f1 = df['Rouge-2 F1'].max()\n",
    "    \n",
    "    # Create styled dataframe\n",
    "    def highlight_best(val, best_val):\n",
    "        if val == best_val:\n",
    "            return 'background-color: #90EE90; font-weight: bold'\n",
    "        return ''\n",
    "    \n",
    "    # Format to 4 decimal places\n",
    "    df_display = df.copy()\n",
    "    for col in df.columns[1:]:\n",
    "        df_display[col] = df[col].apply(lambda x: f'{x:.4f}')\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"                              TABLE 1: COMPLETE ROUGE SCORE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    display(df_display)\n",
    "    \n",
    "    return df\n",
    "\n",
    "results_df_full = create_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64998",
   "metadata": {},
   "source": [
    "## Table 2: Model Ranking by Performance\n",
    "\n",
    "Ranking models based on Rouge-1 and Rouge-2 F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Model Ranking Table\n",
    "def create_ranking_table(results):\n",
    "    \"\"\"Create a ranking table based on F1 scores.\"\"\"\n",
    "    \n",
    "    ranking_data = []\n",
    "    for model_name, scores in results.items():\n",
    "        ranking_data.append({\n",
    "            'Model': model_name.replace('_', ' '),\n",
    "            'Rouge-1 F1': scores['rouge1_fmeasure'].item(),\n",
    "            'Rouge-2 F1': scores['rouge2_fmeasure'].item(),\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(ranking_data)\n",
    "    \n",
    "    # Add rankings\n",
    "    df['Rouge-1 Rank'] = df['Rouge-1 F1'].rank(ascending=False).astype(int)\n",
    "    df['Rouge-2 Rank'] = df['Rouge-2 F1'].rank(ascending=False).astype(int)\n",
    "    df['Average Rank'] = ((df['Rouge-1 Rank'] + df['Rouge-2 Rank']) / 2)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    df = df.sort_values('Average Rank')\n",
    "    \n",
    "    # Format scores\n",
    "    df_display = df.copy()\n",
    "    df_display['Rouge-1 F1'] = df['Rouge-1 F1'].apply(lambda x: f'{x:.4f}')\n",
    "    df_display['Rouge-2 F1'] = df['Rouge-2 F1'].apply(lambda x: f'{x:.4f}')\n",
    "    df_display['Average Rank'] = df['Average Rank'].apply(lambda x: f'{x:.1f}')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"                    TABLE 2: MODEL RANKING BY PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_display[['Model', 'Rouge-1 F1', 'Rouge-1 Rank', 'Rouge-2 F1', 'Rouge-2 Rank', 'Average Rank']])\n",
    "    \n",
    "    return df\n",
    "\n",
    "ranking_df = create_ranking_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fea32",
   "metadata": {},
   "source": [
    "## Table 3: Performance Improvement Analysis\n",
    "\n",
    "Comparing each model's improvement over the baseline (GRU + GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Improvement Over Baseline Analysis\n",
    "def create_improvement_table(results):\n",
    "    \"\"\"Calculate improvement over baseline for each model.\"\"\"\n",
    "    \n",
    "    # Get baseline scores\n",
    "    baseline_r1 = results['Baseline_GRU']['rouge1_fmeasure'].item()\n",
    "    baseline_r2 = results['Baseline_GRU']['rouge2_fmeasure'].item()\n",
    "    \n",
    "    improvement_data = []\n",
    "    for model_name, scores in results.items():\n",
    "        r1_f1 = scores['rouge1_fmeasure'].item()\n",
    "        r2_f1 = scores['rouge2_fmeasure'].item()\n",
    "        \n",
    "        # Calculate percentage improvement\n",
    "        r1_improvement = ((r1_f1 - baseline_r1) / baseline_r1) * 100\n",
    "        r2_improvement = ((r2_f1 - baseline_r2) / baseline_r2) * 100\n",
    "        \n",
    "        improvement_data.append({\n",
    "            'Model': model_name.replace('_', ' '),\n",
    "            'Rouge-1 F1': r1_f1,\n",
    "            'R1 vs Baseline': r1_improvement,\n",
    "            'Rouge-2 F1': r2_f1,\n",
    "            'R2 vs Baseline': r2_improvement,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Format for display\n",
    "    df_display = df.copy()\n",
    "    df_display['Rouge-1 F1'] = df['Rouge-1 F1'].apply(lambda x: f'{x:.4f}')\n",
    "    df_display['Rouge-2 F1'] = df['Rouge-2 F1'].apply(lambda x: f'{x:.4f}')\n",
    "    df_display['R1 vs Baseline'] = df['R1 vs Baseline'].apply(lambda x: f'{x:+.2f}%')\n",
    "    df_display['R2 vs Baseline'] = df['R2 vs Baseline'].apply(lambda x: f'{x:+.2f}%')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"                    TABLE 3: PERFORMANCE IMPROVEMENT OVER BASELINE\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"Baseline Model: GRU Encoder + GRU Decoder\")\n",
    "    print(f\"Baseline Rouge-1 F1: {baseline_r1:.4f} | Baseline Rouge-2 F1: {baseline_r2:.4f}\")\n",
    "    print(\"-\"*90)\n",
    "    display(df_display)\n",
    "    \n",
    "    return df\n",
    "\n",
    "improvement_df = create_improvement_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ff4e7",
   "metadata": {},
   "source": [
    "## Detailed Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis and Detailed Comparison\n",
    "def detailed_analysis(results):\n",
    "    \"\"\"Provide detailed statistical analysis of results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"                              DETAILED ANALYSIS AND COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Extract scores\n",
    "    models = list(results.keys())\n",
    "    r1_scores = [results[m]['rouge1_fmeasure'].item() for m in models]\n",
    "    r2_scores = [results[m]['rouge2_fmeasure'].item() for m in models]\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"1. STATISTICAL SUMMARY\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"\\nRouge-1 F1 Scores:\")\n",
    "    print(f\"   Mean:     {np.mean(r1_scores):.4f}\")\n",
    "    print(f\"   Std Dev:  {np.std(r1_scores):.4f}\")\n",
    "    print(f\"   Min:      {np.min(r1_scores):.4f} ({models[np.argmin(r1_scores)]})\")\n",
    "    print(f\"   Max:      {np.max(r1_scores):.4f} ({models[np.argmax(r1_scores)]})\")\n",
    "    print(f\"   Range:    {np.max(r1_scores) - np.min(r1_scores):.4f}\")\n",
    "    \n",
    "    print(f\"\\nRouge-2 F1 Scores:\")\n",
    "    print(f\"   Mean:     {np.mean(r2_scores):.4f}\")\n",
    "    print(f\"   Std Dev:  {np.std(r2_scores):.4f}\")\n",
    "    print(f\"   Min:      {np.min(r2_scores):.4f} ({models[np.argmin(r2_scores)]})\")\n",
    "    print(f\"   Max:      {np.max(r2_scores):.4f} ({models[np.argmax(r2_scores)]})\")\n",
    "    print(f\"   Range:    {np.max(r2_scores) - np.min(r2_scores):.4f}\")\n",
    "    \n",
    "    # Best model identification\n",
    "    best_r1_idx = np.argmax(r1_scores)\n",
    "    best_r2_idx = np.argmax(r2_scores)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"2. BEST PERFORMING MODELS\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"\\n   Best Rouge-1 F1: {models[best_r1_idx]} ({r1_scores[best_r1_idx]:.4f})\")\n",
    "    print(f\"   Best Rouge-2 F1: {models[best_r2_idx]} ({r2_scores[best_r2_idx]:.4f})\")\n",
    "    \n",
    "    if best_r1_idx == best_r2_idx:\n",
    "        print(f\"\\n    Overall Best Model: {models[best_r1_idx]}\")\n",
    "    else:\n",
    "        print(f\"\\n   Note: Different models excel at different metrics\")\n",
    "    \n",
    "    return r1_scores, r2_scores\n",
    "\n",
    "r1_scores, r2_scores = detailed_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d9be2",
   "metadata": {},
   "source": [
    "## Pairwise Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Comparison Between Models\n",
    "def pairwise_comparison(results):\n",
    "    \"\"\"Create pairwise comparison analysis.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"                              PAIRWISE MODEL COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    comparisons = [\n",
    "        (\"Baseline_GRU\", \"LSTM_LSTM\", \"GRU vs LSTM\", \n",
    "         \"LSTM includes cell state (memory) in addition to hidden state\"),\n",
    "        (\"Baseline_GRU\", \"BiLSTM_GRU\", \"Unidirectional vs Bidirectional Encoder\",\n",
    "         \"Bidirectional captures context from both past and future tokens\"),\n",
    "        (\"Baseline_GRU\", \"GRU_Attention\", \"Without vs With Attention\",\n",
    "         \"Attention allows decoder to focus on relevant encoder states\"),\n",
    "        (\"Baseline_GRU\", \"Transformer_GRU\", \"RNN vs Transformer Encoder\",\n",
    "         \"Transformer uses self-attention for parallel processing\"),\n",
    "        (\"GRU_Attention\", \"Transformer_GRU\", \"RNN Attention vs Transformer\",\n",
    "         \"Different attention mechanisms: encoder-decoder vs self-attention\"),\n",
    "    ]\n",
    "    \n",
    "    for model1, model2, comparison_name, description in comparisons:\n",
    "        if model1 in results and model2 in results:\n",
    "            r1_diff = results[model2]['rouge1_fmeasure'].item() - results[model1]['rouge1_fmeasure'].item()\n",
    "            r2_diff = results[model2]['rouge2_fmeasure'].item() - results[model1]['rouge2_fmeasure'].item()\n",
    "            \n",
    "            winner = model2 if r1_diff > 0 else model1\n",
    "            \n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Comparison: {comparison_name}\")\n",
    "            print(f\"Description: {description}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            print(f\"   {model1.replace('_', ' ')}:\")\n",
    "            print(f\"      Rouge-1 F1: {results[model1]['rouge1_fmeasure'].item():.4f}\")\n",
    "            print(f\"      Rouge-2 F1: {results[model1]['rouge2_fmeasure'].item():.4f}\")\n",
    "            print(f\"   {model2.replace('_', ' ')}:\")\n",
    "            print(f\"      Rouge-1 F1: {results[model2]['rouge1_fmeasure'].item():.4f}\")\n",
    "            print(f\"      Rouge-2 F1: {results[model2]['rouge2_fmeasure'].item():.4f}\")\n",
    "            print(f\"\\n   Difference (Model2 - Model1):\")\n",
    "            print(f\"      Rouge-1 F1: {r1_diff:+.4f} ({r1_diff/results[model1]['rouge1_fmeasure'].item()*100:+.2f}%)\")\n",
    "            print(f\"      Rouge-2 F1: {r2_diff:+.4f} ({r2_diff/results[model1]['rouge2_fmeasure'].item()*100:+.2f}%)\")\n",
    "            print(f\"\\n    Winner: {winner.replace('_', ' ')}\")\n",
    "\n",
    "pairwise_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec86ef",
   "metadata": {},
   "source": [
    "## Visualization: Detailed Performance Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef07602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Visualization\n",
    "def create_detailed_visualizations(results):\n",
    "    \"\"\"Create comprehensive visualizations for the report.\"\"\"\n",
    "    \n",
    "    models = list(results.keys())\n",
    "    model_labels = [m.replace('_', '\\n') for m in models]\n",
    "    \n",
    "    # Extract all metrics\n",
    "    r1_f1 = [results[m]['rouge1_fmeasure'].item() for m in models]\n",
    "    r1_prec = [results[m]['rouge1_precision'].item() for m in models]\n",
    "    r1_rec = [results[m]['rouge1_recall'].item() for m in models]\n",
    "    r2_f1 = [results[m]['rouge2_fmeasure'].item() for m in models]\n",
    "    r2_prec = [results[m]['rouge2_precision'].item() for m in models]\n",
    "    r2_rec = [results[m]['rouge2_recall'].item() for m in models]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Plot 1: Rouge-1 All Metrics\n",
    "    ax1 = axes[0, 0]\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    ax1.bar(x - width, r1_f1, width, label='F1', color='#2ecc71')\n",
    "    ax1.bar(x, r1_prec, width, label='Precision', color='#3498db')\n",
    "    ax1.bar(x + width, r1_rec, width, label='Recall', color='#e74c3c')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Rouge-1 Scores by Model')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(model_labels, fontsize=8)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Rouge-2 All Metrics\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.bar(x - width, r2_f1, width, label='F1', color='#2ecc71')\n",
    "    ax2.bar(x, r2_prec, width, label='Precision', color='#3498db')\n",
    "    ax2.bar(x + width, r2_rec, width, label='Recall', color='#e74c3c')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Rouge-2 Scores by Model')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(model_labels, fontsize=8)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: F1 Score Comparison (Horizontal Bar)\n",
    "    ax3 = axes[1, 0]\n",
    "    y_pos = np.arange(len(models))\n",
    "    ax3.barh(y_pos - 0.2, r1_f1, 0.4, label='Rouge-1 F1', color='steelblue')\n",
    "    ax3.barh(y_pos + 0.2, r2_f1, 0.4, label='Rouge-2 F1', color='coral')\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels([m.replace('_', ' ') for m in models])\n",
    "    ax3.set_xlabel('F1 Score')\n",
    "    ax3.set_title('F1 Score Comparison')\n",
    "    ax3.legend(loc='lower right')\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (v1, v2) in enumerate(zip(r1_f1, r2_f1)):\n",
    "        ax3.text(v1 + 0.01, i - 0.2, f'{v1:.3f}', va='center', fontsize=8)\n",
    "        ax3.text(v2 + 0.01, i + 0.2, f'{v2:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    # Plot 4: Radar/Spider Chart (simplified as grouped comparison)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate improvement over baseline\n",
    "    baseline_r1 = results['Baseline_GRU']['rouge1_fmeasure'].item()\n",
    "    baseline_r2 = results['Baseline_GRU']['rouge2_fmeasure'].item()\n",
    "    \n",
    "    improvements_r1 = [(results[m]['rouge1_fmeasure'].item() - baseline_r1) / baseline_r1 * 100 for m in models]\n",
    "    improvements_r2 = [(results[m]['rouge2_fmeasure'].item() - baseline_r2) / baseline_r2 * 100 for m in models]\n",
    "    \n",
    "    colors = ['green' if x >= 0 else 'red' for x in improvements_r1]\n",
    "    ax4.barh(y_pos - 0.2, improvements_r1, 0.4, label='Rouge-1 Improvement', color=['#27ae60' if x >= 0 else '#e74c3c' for x in improvements_r1], alpha=0.7)\n",
    "    ax4.barh(y_pos + 0.2, improvements_r2, 0.4, label='Rouge-2 Improvement', color=['#2980b9' if x >= 0 else '#c0392b' for x in improvements_r2], alpha=0.7)\n",
    "    ax4.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels([m.replace('_', ' ') for m in models])\n",
    "    ax4.set_xlabel('Improvement over Baseline (%)')\n",
    "    ax4.set_title('Percentage Improvement vs Baseline')\n",
    "    ax4.legend(loc='lower right')\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detailed_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Visualization saved as 'detailed_analysis.png'\")\n",
    "\n",
    "create_detailed_visualizations(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abeb912",
   "metadata": {},
   "source": [
    "## Explanation of Findings\n",
    "\n",
    "This section provides theoretical explanations for the observed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c55533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Explanation of Findings\n",
    "def generate_findings_explanation(results):\n",
    "    \"\"\"Generate detailed explanations based on experimental results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"                              EXPLANATION OF FINDINGS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Get scores for analysis\n",
    "    baseline_r1 = results['Baseline_GRU']['rouge1_fmeasure'].item()\n",
    "    baseline_r2 = results['Baseline_GRU']['rouge2_fmeasure'].item()\n",
    "    \n",
    "    explanations = {\n",
    "        'Baseline_GRU': {\n",
    "            'name': 'Baseline: GRU Encoder + GRU Decoder',\n",
    "            'description': '''\n",
    "The baseline model uses GRU (Gated Recurrent Unit) for both encoder and decoder.\n",
    "\n",
    "Key Characteristics:\n",
    " GRU has two gates: reset gate and update gate\n",
    " Simpler than LSTM with fewer parameters\n",
    " Encodes the entire input sequence into a single context vector\n",
    " Decoder generates output based solely on this fixed-size context vector\n",
    "\n",
    "Limitations:\n",
    " Information bottleneck: entire input compressed into one vector\n",
    " May struggle with long sequences\n",
    " No direct access to individual encoder states during decoding\n",
    "'''\n",
    "        },\n",
    "        'LSTM_LSTM': {\n",
    "            'name': 'LSTM Encoder + LSTM Decoder',\n",
    "            'description': '''\n",
    "This model replaces GRU with LSTM (Long Short-Term Memory) units.\n",
    "\n",
    "Key Differences from GRU:\n",
    " LSTM has three gates: input, forget, and output gates\n",
    " Includes explicit cell state for long-term memory\n",
    " Better at capturing long-range dependencies\n",
    " More parameters  potentially better expressiveness\n",
    "\n",
    "Expected Impact:\n",
    " May improve performance on longer sequences\n",
    " Cell state helps maintain relevant information over time\n",
    " Slightly slower training due to more parameters\n",
    "'''\n",
    "        },\n",
    "        'BiLSTM_GRU': {\n",
    "            'name': 'Bidirectional LSTM Encoder + GRU Decoder',\n",
    "            'description': '''\n",
    "This model uses a bidirectional LSTM encoder while keeping GRU decoder.\n",
    "\n",
    "Key Characteristics:\n",
    " Processes input in both forward and backward directions\n",
    " Captures context from both past and future tokens\n",
    " Concatenates forward and backward hidden states\n",
    " Provides richer representation of each input token\n",
    "\n",
    "Expected Impact:\n",
    " Better understanding of context from both directions\n",
    " Particularly helpful for translation where word order differs\n",
    " May improve handling of syntactic structures\n",
    "'''\n",
    "        },\n",
    "        'GRU_Attention': {\n",
    "            'name': 'GRU Encoder + GRU Decoder with Attention',\n",
    "            'description': '''\n",
    "This model adds Bahdanau-style attention mechanism between encoder and decoder.\n",
    "\n",
    "Attention Mechanism Details:\n",
    " Query: decoder hidden state s_t\n",
    " Keys/Values: all encoder hidden states h_i\n",
    " Score function: additive (Bahdanau): v^T * tanh(W_s*s_t + W_h*h_i)\n",
    " Attention weights: softmax(scores)\n",
    " Context vector: weighted sum of encoder states\n",
    "\n",
    "Expected Impact:\n",
    " Eliminates information bottleneck problem\n",
    " Decoder can directly access all encoder states\n",
    " Different encoder states get different attention at each decoding step\n",
    " Particularly effective for translation tasks\n",
    " Provides interpretable alignment between source and target\n",
    "'''\n",
    "        },\n",
    "        'Transformer_GRU': {\n",
    "            'name': 'Transformer Encoder + GRU Decoder',\n",
    "            'description': '''\n",
    "This model replaces RNN encoder with Transformer encoder.\n",
    "\n",
    "Key Characteristics:\n",
    " Self-attention mechanism for encoding\n",
    " Parallel processing of all input tokens\n",
    " Positional encoding for sequence order information\n",
    " Multi-head attention captures different aspects of relationships\n",
    " Mean pooling of all token representations as sentence embedding\n",
    "\n",
    "Expected Impact:\n",
    " Better at capturing global dependencies\n",
    " May require more data for optimal performance\n",
    " Faster training due to parallelization\n",
    " Different inductive bias compared to RNNs\n",
    "'''\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for model_key, info in explanations.items():\n",
    "        if model_key in results:\n",
    "            r1 = results[model_key]['rouge1_fmeasure'].item()\n",
    "            r2 = results[model_key]['rouge2_fmeasure'].item()\n",
    "            r1_diff = ((r1 - baseline_r1) / baseline_r1) * 100\n",
    "            r2_diff = ((r2 - baseline_r2) / baseline_r2) * 100\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"MODEL: {info['name']}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(info['description'])\n",
    "            print(f\"\\nActual Results:\")\n",
    "            print(f\"   Rouge-1 F1: {r1:.4f} ({r1_diff:+.2f}% vs baseline)\")\n",
    "            print(f\"   Rouge-2 F1: {r2:.4f} ({r2_diff:+.2f}% vs baseline)\")\n",
    "\n",
    "generate_findings_explanation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f2607",
   "metadata": {},
   "source": [
    "## Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188764ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Report\n",
    "def generate_final_report(results):\n",
    "    \"\"\"Generate a comprehensive final report.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"                              FINAL SUMMARY REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Get best models\n",
    "    models = list(results.keys())\n",
    "    r1_scores = {m: results[m]['rouge1_fmeasure'].item() for m in models}\n",
    "    r2_scores = {m: results[m]['rouge2_fmeasure'].item() for m in models}\n",
    "    \n",
    "    best_r1 = max(r1_scores, key=r1_scores.get)\n",
    "    best_r2 = max(r2_scores, key=r2_scores.get)\n",
    "    \n",
    "    # Calculate average rank\n",
    "    r1_ranks = {m: sorted(r1_scores.values(), reverse=True).index(r1_scores[m]) + 1 for m in models}\n",
    "    r2_ranks = {m: sorted(r2_scores.values(), reverse=True).index(r2_scores[m]) + 1 for m in models}\n",
    "    avg_ranks = {m: (r1_ranks[m] + r2_ranks[m]) / 2 for m in models}\n",
    "    overall_best = min(avg_ranks, key=avg_ranks.get)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "\n",
    "                           EXPERIMENT SUMMARY                                    \n",
    "\n",
    "  Total Models Evaluated: {len(models)}                                                       \n",
    "  Evaluation Metric: ROUGE Score (Rouge-1 and Rouge-2)                          \n",
    "  Dataset: French-English Translation (Tatoeba)                                 \n",
    "  Training Epochs: 5                                                            \n",
    "\n",
    "\n",
    "\n",
    "                              KEY FINDINGS                                       \n",
    "\n",
    "  Best Rouge-1 F1: {best_r1.replace('_', ' '):<25} Score: {r1_scores[best_r1]:.4f}          \n",
    "  Best Rouge-2 F1: {best_r2.replace('_', ' '):<25} Score: {r2_scores[best_r2]:.4f}          \n",
    "  Overall Best:    {overall_best.replace('_', ' '):<25} Avg Rank: {avg_ranks[overall_best]:.1f}            \n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\"\"\n",
    "\n",
    "                           KEY INSIGHTS                                          \n",
    "\n",
    "                                                                                \n",
    "  1. ATTENTION MECHANISM                                                        \n",
    "     The attention mechanism allows the decoder to focus on relevant parts      \n",
    "     of the input sequence at each decoding step, addressing the information    \n",
    "     bottleneck problem inherent in basic seq2seq models.                       \n",
    "                                                                                \n",
    "  2. BIDIRECTIONAL ENCODING                                                     \n",
    "     Bidirectional LSTM captures context from both directions, which can        \n",
    "     improve understanding of the source sentence, especially when word         \n",
    "     order differs between languages.                                           \n",
    "                                                                                \n",
    "  3. LSTM vs GRU                                                                \n",
    "     LSTM's explicit cell state may help with longer sequences, though the      \n",
    "     difference may be minimal for shorter sentences in our filtered dataset.   \n",
    "                                                                                \n",
    "  4. TRANSFORMER ENCODER                                                        \n",
    "     Transformer's self-attention can capture global dependencies but may       \n",
    "     require more data and careful hyperparameter tuning for optimal results.   \n",
    "                                                                                \n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # Final ranking table\n",
    "    print(\"\\n\")\n",
    "    print(\"                           FINAL MODEL RANKING                                  \")\n",
    "    print(\"\")\n",
    "    \n",
    "    sorted_models = sorted(avg_ranks.keys(), key=lambda x: avg_ranks[x])\n",
    "    for rank, model in enumerate(sorted_models, 1):\n",
    "        medal = \"\" if rank == 1 else \"\" if rank == 2 else \"\" if rank == 3 else \"  \"\n",
    "        print(f\"  {medal} Rank {rank}: {model.replace('_', ' '):<30} R1: {r1_scores[model]:.4f}  R2: {r2_scores[model]:.4f} \")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "generate_final_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results to CSV for Report\n",
    "def export_results_to_csv(results):\n",
    "    \"\"\"Export all results to CSV files for inclusion in reports.\"\"\"\n",
    "    \n",
    "    # Full results table\n",
    "    full_data = []\n",
    "    for model_name, scores in results.items():\n",
    "        full_data.append({\n",
    "            'Model': model_name.replace('_', ' '),\n",
    "            'Rouge-1 F1': round(scores['rouge1_fmeasure'].item(), 4),\n",
    "            'Rouge-1 Precision': round(scores['rouge1_precision'].item(), 4),\n",
    "            'Rouge-1 Recall': round(scores['rouge1_recall'].item(), 4),\n",
    "            'Rouge-2 F1': round(scores['rouge2_fmeasure'].item(), 4),\n",
    "            'Rouge-2 Precision': round(scores['rouge2_precision'].item(), 4),\n",
    "            'Rouge-2 Recall': round(scores['rouge2_recall'].item(), 4),\n",
    "        })\n",
    "    \n",
    "    df_full = pd.DataFrame(full_data)\n",
    "    df_full.to_csv('results_full.csv', index=False)\n",
    "    \n",
    "    # Summary table (F1 only)\n",
    "    summary_data = []\n",
    "    baseline_r1 = results['Baseline_GRU']['rouge1_fmeasure'].item()\n",
    "    baseline_r2 = results['Baseline_GRU']['rouge2_fmeasure'].item()\n",
    "    \n",
    "    for model_name, scores in results.items():\n",
    "        r1 = scores['rouge1_fmeasure'].item()\n",
    "        r2 = scores['rouge2_fmeasure'].item()\n",
    "        summary_data.append({\n",
    "            'Model': model_name.replace('_', ' '),\n",
    "            'Rouge-1 F1': round(r1, 4),\n",
    "            'Rouge-2 F1': round(r2, 4),\n",
    "            'R1 Improvement (%)': round(((r1 - baseline_r1) / baseline_r1) * 100, 2),\n",
    "            'R2 Improvement (%)': round(((r2 - baseline_r2) / baseline_r2) * 100, 2),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    df_summary.to_csv('results_summary.csv', index=False)\n",
    "    \n",
    "    print(\" Results exported to:\")\n",
    "    print(\"   - results_full.csv (all metrics)\")\n",
    "    print(\"   - results_summary.csv (F1 scores with improvements)\")\n",
    "    print(\"\\nYou can use these CSV files in your report!\")\n",
    "    \n",
    "    return df_full, df_summary\n",
    "\n",
    "df_full, df_summary = export_results_to_csv(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary Table (for copy-paste into report):\")\n",
    "print(\"=\"*60)\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3dfdb",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Summary of Experiments\n",
    "\n",
    "This project implemented and compared five different sequence-to-sequence architectures for French-to-English translation:\n",
    "\n",
    "| Task | Model | Encoder | Decoder | Key Feature |\n",
    "|------|-------|---------|---------|-------------|\n",
    "| 2 | Baseline | GRU | GRU | Standard seq2seq |\n",
    "| 3 | LSTM | LSTM | LSTM | Cell state for long-term memory |\n",
    "| 4 | BiLSTM | Bi-LSTM | GRU | Bidirectional context encoding |\n",
    "| 5 | Attention | GRU | GRU + Attention | Bahdanau attention mechanism |\n",
    "| 6 | Transformer | Transformer | GRU | Self-attention encoding |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Attention Mechanism**: Adding attention typically improves translation quality by allowing the decoder to directly access relevant encoder states at each timestep, solving the information bottleneck problem.\n",
    "\n",
    "2. **Bidirectional Encoding**: Bi-LSTM captures context from both directions, which is particularly useful for translation where word order may differ between languages.\n",
    "\n",
    "3. **LSTM vs GRU**: Both architectures perform similarly on this task. LSTM's explicit cell state may provide marginal benefits for longer sequences.\n",
    "\n",
    "4. **Transformer Encoder**: While powerful, the Transformer may require more training data and careful hyperparameter tuning to outperform attention-based RNN models on smaller datasets.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- For **production systems**: Use attention-based models for best translation quality\n",
    "- For **limited computational resources**: GRU-based models offer good balance of speed and performance\n",
    "- For **longer sequences**: Consider Bi-LSTM encoder with attention\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Experiment with different attention mechanisms (Luong vs Bahdanau)\n",
    "- Try full Transformer architecture (encoder + decoder)\n",
    "- Implement beam search for decoding\n",
    "- Train on larger datasets for more robust comparisons"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
