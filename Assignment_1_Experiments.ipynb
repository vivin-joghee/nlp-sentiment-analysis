{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26ba4fd",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.4.0 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c34aea",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3428f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  tokenizer_language='en_core_web_sm',\n",
    "                  include_lengths=True,\n",
    "                  pad_first=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb435db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset using Hugging Face\n",
    "from datasets import load_dataset\n",
    "from torchtext.data import Example, Dataset\n",
    "\n",
    "print(\"Loading IMDB dataset from Hugging Face...\")\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Convert to torchtext format\n",
    "train_examples = [Example.fromlist([item['text'], item['label']], \n",
    "                                   [('text', TEXT), ('label', LABEL)]) \n",
    "                 for item in imdb['train']]\n",
    "\n",
    "test_examples = [Example.fromlist([item['text'], item['label']], \n",
    "                                  [('text', TEXT), ('label', LABEL)]) \n",
    "                for item in imdb['test']]\n",
    "\n",
    "train_data = Dataset(train_examples, [('text', TEXT), ('label', LABEL)])\n",
    "test_data = Dataset(test_examples, [('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples and {len(test_data)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation\n",
    "import random\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28276870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f'Created iterators with batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d80a0b",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "We'll define all required model architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic RNN Model (for Task 1 and 2)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feed-Forward Neural Networks\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims, output_dim, dropout=0.5):\n",
    "        \"\"\"\n",
    "        hidden_dims: list of hidden layer dimensions\n",
    "        e.g., [500] for 1-layer, [500, 300] for 2-layer, [500, 300, 200] for 3-layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        prev_dim = embedding_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)  # [sent len, batch size, emb dim]\n",
    "        # Take mean of embeddings across sequence length\n",
    "        pooled = embedded.mean(dim=0)  # [batch size, emb dim]\n",
    "        return self.layers(pooled).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9855a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        \"\"\"\n",
    "        filter_sizes: list of kernel sizes, e.g., [1, 2, 3]\n",
    "        n_filters: number of filters for each kernel size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Create convolutional layers for each filter size\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, \n",
    "                     kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)  # [sent len, batch size, emb dim]\n",
    "        embedded = embedded.permute(1, 0, 2)  # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        # Apply convolutions and max pooling\n",
    "        conved = [nn.functional.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, \n",
    "                 bidirectional=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # hidden = [num layers * num directions, batch size, hidden dim]\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        \n",
    "        return self.fc(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e9bef",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b65ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"Calculate accuracy\"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09f9d8",
   "metadata": {},
   "source": [
    "## Experiment Runner\n",
    "\n",
    "Function to run experiments and track results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, optimizer, n_epochs, experiment_name):\n",
    "    \"\"\"\n",
    "    Run a complete experiment and return results\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        training_time += (end_time - start_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'best-model.pt')\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tVal. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(torch.load('best-model.pt'))\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    print(f'Total Training Time: {training_time/60:.2f} minutes')\n",
    "    \n",
    "    return {\n",
    "        'experiment': experiment_name,\n",
    "        'best_valid_loss': best_valid_loss,\n",
    "        'best_valid_acc': best_valid_acc * 100,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc * 100,\n",
    "        'training_time': training_time / 60,  # in minutes\n",
    "        'n_params': count_parameters(model)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3ed0d",
   "metadata": {},
   "source": [
    "## Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f929810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all results\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62d0c3",
   "metadata": {},
   "source": [
    "## Task 1: Warmup - Baseline RNN with SGD\n",
    "\n",
    "This is the baseline experiment from the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Create model\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Run experiment\n",
    "result = run_experiment(model, optimizer, n_epochs=20, \n",
    "                       experiment_name='Task 1: RNN with SGD (Baseline)')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7cb3d",
   "metadata": {},
   "source": [
    "## Task 2: Different Optimizers (SGD, Adam, Adagrad)\n",
    "\n",
    "Compare performance with different optimizers using the same RNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2a: RNN with Adam\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=20,\n",
    "                       experiment_name='Task 2: RNN with Adam')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2b: RNN with Adagrad\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adagrad(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=20,\n",
    "                       experiment_name='Task 2: RNN with Adagrad')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5208fb",
   "metadata": {},
   "source": [
    "## Task 3: Different Number of Epochs with Adam\n",
    "\n",
    "Test with 5, 10, 20, and 50 epochs using Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c71c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3a: 5 epochs\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=5,\n",
    "                       experiment_name='Task 3: RNN Adam - 5 epochs')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3b: 10 epochs\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=10,\n",
    "                       experiment_name='Task 3: RNN Adam - 10 epochs')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3c: 20 epochs (already done above, but for completeness)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=20,\n",
    "                       experiment_name='Task 3: RNN Adam - 20 epochs')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3d: 50 epochs\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 3: RNN Adam - 50 epochs')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc98a1",
   "metadata": {},
   "source": [
    "## Task 4: Different Model Architectures\n",
    "\n",
    "All experiments use Adam optimizer, 50 epochs, and randomly initialized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4a: One-layer Feed-Forward NN (hidden_dim=500)\n",
    "model = FeedForwardNN(INPUT_DIM, EMBEDDING_DIM, [500], OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: 1-Layer FFN (500)')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4b: Two-layer Feed-Forward NN (hidden_dims=500, 300)\n",
    "model = FeedForwardNN(INPUT_DIM, EMBEDDING_DIM, [500, 300], OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: 2-Layer FFN (500, 300)')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d992620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4c: Three-layer Feed-Forward NN (hidden_dims=500, 300, 200)\n",
    "model = FeedForwardNN(INPUT_DIM, EMBEDDING_DIM, [500, 300, 200], OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: 3-Layer FFN (500, 300, 200)')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4d: CNN with filter sizes [1, 2, 3]\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [1, 2, 3]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: CNN (filters 1,2,3)')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4e: LSTM\n",
    "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=2, bidirectional=False)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: LSTM')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4f: Bi-LSTM\n",
    "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=2, bidirectional=True)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "result = run_experiment(model, optimizer, n_epochs=50,\n",
    "                       experiment_name='Task 4: Bi-LSTM')\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532947d",
   "metadata": {},
   "source": [
    "## Results Summary and Analysis\n",
    "\n",
    "Let's create comprehensive tables for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display complete results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPLETE EXPERIMENTAL RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'experiment_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62991812",
   "metadata": {},
   "source": [
    "### Table 1: Optimizer Comparison (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results for optimizer comparison\n",
    "optimizer_results = results_df[results_df['experiment'].str.contains('Task 2|Task 1')].copy()\n",
    "optimizer_results['Optimizer'] = optimizer_results['experiment'].str.extract(r'with (\\w+)')\n",
    "\n",
    "table1 = optimizer_results[['Optimizer', 'test_acc', 'test_loss', 'training_time']].copy()\n",
    "table1.columns = ['Optimizer', 'Test Accuracy (%)', 'Test Loss', 'Training Time (min)']\n",
    "table1 = table1.round(2)\n",
    "\n",
    "print(\"\\nTable 1: Optimizer Comparison (RNN, 20 epochs)\")\n",
    "print(\"=\"*60)\n",
    "print(table1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723e6dd",
   "metadata": {},
   "source": [
    "### Table 2: Epoch Comparison (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results for epoch comparison\n",
    "epoch_results = results_df[results_df['experiment'].str.contains('Task 3')].copy()\n",
    "epoch_results['Epochs'] = epoch_results['experiment'].str.extract(r'(\\d+) epochs')\n",
    "\n",
    "table2 = epoch_results[['Epochs', 'test_acc', 'test_loss', 'training_time']].copy()\n",
    "table2.columns = ['Epochs', 'Test Accuracy (%)', 'Test Loss', 'Training Time (min)']\n",
    "table2 = table2.round(2)\n",
    "\n",
    "print(\"\\nTable 2: Epoch Comparison (RNN with Adam)\")\n",
    "print(\"=\"*60)\n",
    "print(table2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016d66c",
   "metadata": {},
   "source": [
    "### Table 3: Model Architecture Comparison (Task 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results for model comparison\n",
    "model_results = results_df[results_df['experiment'].str.contains('Task 4')].copy()\n",
    "model_results['Model'] = model_results['experiment'].str.replace('Task 4: ', '')\n",
    "\n",
    "table3 = model_results[['Model', 'n_params', 'test_acc', 'test_loss', 'training_time']].copy()\n",
    "table3.columns = ['Model Architecture', 'Parameters', 'Test Accuracy (%)', 'Test Loss', 'Training Time (min)']\n",
    "table3['Parameters'] = table3['Parameters'].apply(lambda x: f\"{x:,}\")\n",
    "table3 = table3.round(2)\n",
    "\n",
    "print(\"\\nTable 3: Model Architecture Comparison (Adam, 50 epochs, Random Embeddings)\")\n",
    "print(\"=\"*100)\n",
    "print(table3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7446d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20945c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 1: Optimizer Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "optimizer_data = results_df[results_df['experiment'].str.contains('Task 2|Task 1')]\n",
    "optimizers = optimizer_data['experiment'].str.extract(r'with (\\w+)')[0].values\n",
    "\n",
    "axes[0].bar(optimizers, optimizer_data['test_acc'])\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('Optimizer Comparison - Test Accuracy')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(optimizers, optimizer_data['training_time'])\n",
    "axes[1].set_ylabel('Training Time (minutes)')\n",
    "axes[1].set_title('Optimizer Comparison - Training Time')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimizer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Epoch Comparison\n",
    "epoch_data = results_df[results_df['experiment'].str.contains('Task 3')]\n",
    "epochs = epoch_data['experiment'].str.extract(r'(\\d+) epochs')[0].astype(int).values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(epochs, epoch_data['test_acc'], 'o-', linewidth=2, markersize=8, label='Test Accuracy')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Impact of Training Epochs on Test Accuracy (RNN with Adam)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('epoch_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Model Architecture Comparison\n",
    "model_data = results_df[results_df['experiment'].str.contains('Task 4')]\n",
    "model_names = model_data['experiment'].str.replace('Task 4: ', '').values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(range(len(model_names)), model_data['test_acc'])\n",
    "ax.set_xticks(range(len(model_names)))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Model Architecture Comparison (Adam, 50 epochs)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Color the best model\n",
    "best_idx = model_data['test_acc'].idxmax() - model_data.index[0]\n",
    "bars[best_idx].set_color('green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c3bcb",
   "metadata": {},
   "source": [
    "## Key Findings and Analysis\n",
    "\n",
    "**For your report, consider these analysis points:**\n",
    "\n",
    "### 1. Optimizer Comparison (Task 2)\n",
    "- Compare SGD vs Adam vs Adagrad\n",
    "- Discuss convergence speed and final accuracy\n",
    "- Explain why Adam typically performs better (adaptive learning rates)\n",
    "\n",
    "### 2. Effect of Training Epochs (Task 3)\n",
    "- Show how accuracy improves with more epochs\n",
    "- Discuss diminishing returns and potential overfitting\n",
    "- Identify the optimal number of epochs\n",
    "\n",
    "### 3. Model Architecture Comparison (Task 4)\n",
    "- Compare simple FFNs vs sequential models (LSTM, Bi-LSTM)\n",
    "- Discuss why LSTM/Bi-LSTM capture sequential information better\n",
    "- Analyze the trade-off between model complexity and performance\n",
    "- Explain CNN's effectiveness in capturing local patterns\n",
    "\n",
    "### 4. General Observations\n",
    "- All models use randomly initialized embeddings (not Word2Vec)\n",
    "- If you had Word2Vec results, you could compare:\n",
    "  - Pre-trained embeddings capture semantic relationships\n",
    "  - Random embeddings learn task-specific representations\n",
    "  - Pre-trained usually gives better performance with less data\n",
    "\n",
    "### 5. Recommendations\n",
    "- Best optimizer for this task\n",
    "- Optimal training duration\n",
    "- Most effective model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a223ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing configurations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_overall = results_df.loc[results_df['test_acc'].idxmax()]\n",
    "print(f\"\\nBest Overall Performance:\")\n",
    "print(f\"  Experiment: {best_overall['experiment']}\")\n",
    "print(f\"  Test Accuracy: {best_overall['test_acc']:.2f}%\")\n",
    "print(f\"  Test Loss: {best_overall['test_loss']:.3f}\")\n",
    "print(f\"  Training Time: {best_overall['training_time']:.2f} minutes\")\n",
    "\n",
    "# Best by category\n",
    "print(\"\\nBest by Task:\")\n",
    "for task in ['Task 1', 'Task 2', 'Task 3', 'Task 4']:\n",
    "    task_data = results_df[results_df['experiment'].str.contains(task)]\n",
    "    if len(task_data) > 0:\n",
    "        best = task_data.loc[task_data['test_acc'].idxmax()]\n",
    "        print(f\"\\n  {task}: {best['experiment']}\")\n",
    "        print(f\"    Test Accuracy: {best['test_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378370f",
   "metadata": {},
   "source": [
    "## Report Writing Tips\n",
    "\n",
    "Use the tables and plots above to create your report. Here's a suggested structure:\n",
    "\n",
    "### 1. Introduction (0.5 pages)\n",
    "- Brief overview of sentiment analysis task\n",
    "- Description of IMDB dataset\n",
    "- Objectives of the experiments\n",
    "\n",
    "### 2. Methodology (1 page)\n",
    "- Data preprocessing\n",
    "- Model architectures (briefly describe each)\n",
    "- Training procedure and hyperparameters\n",
    "- Evaluation metrics\n",
    "\n",
    "### 3. Results (1.5 pages)\n",
    "- Present Tables 1, 2, and 3\n",
    "- Include the comparison plots\n",
    "- Report best performing configurations\n",
    "\n",
    "### 4. Analysis and Discussion (1 page)\n",
    "- **Optimizer comparison**: Why does Adam outperform SGD?\n",
    "- **Epoch analysis**: Does more training always help?\n",
    "- **Architecture comparison**: Why do LSTMs work better for sequences?\n",
    "- **Random vs Pre-trained embeddings**: What would be the expected difference?\n",
    "\n",
    "### 5. Conclusion (0.5 pages)\n",
    "- Summary of key findings\n",
    "- Recommendations for sentiment analysis tasks\n",
    "- Future work suggestions\n",
    "\n",
    "**Note**: Keep it concise and focus on insights rather than just reporting numbers!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
